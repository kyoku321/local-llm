# Common configuration for all vLLM instances
common:
  image:
    repository: vllm/vllm-openai
    tag: v0.12.0
    pullPolicy: IfNotPresent

  persistence:
    enabled: true
    storageClass: "basic"
    accessModes:
      - ReadWriteMany
    size: 800Gi
    mountPath: /models

  service:
    type: ClusterIP

  # Global HuggingFace Token
  hfToken: "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"

# Individual vLLM service definitions
# You can add more services here by adding a new key
services:
  # LLM Model Service
  llm:
    enabled: true
    replicaCount: 1
    model:
      name: "openai/gpt-oss-20b"
      path: "/models/gpt-oss-20b"
    resources:
      limits:
        nvidia.com/gpu: 1
    nodeSelector:
      memorysize: "80GB"
    service:
      port: 8000
    # Additional arguments for vLLM serve command
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--max-num-batched-tokens"
      - "1024"
      - "--max-model-len"
      - "10240"
      - "--max-num-seqs"
      - "128"
      - "--async-scheduling"
      - "--dtype"
      - "bfloat16"

  # Embedding Model Service
  embedding:
    enabled: true
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-Embedding-8B"
      path: "/models/Qwen3-Embedding-8B"
    resources:
      limits:
        nvidia.com/gpu: 1
    nodeSelector:
      memorysize: "32GB"
    service:
      port: 8001
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "512"
      - "--gpu-memory-utilization"
      - "0.9"
      - "--task"
      - "embed"
      - "--dtype"
      - "float16"

  # Qwen3-235B Model Service
  qwen:
    enabled: false
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
      path: "/models/Qwen3-235B-A22B-Instruct-2507-FP8"
    resources:
      limits:
        nvidia.com/gpu: 4
    service:
      port: 8002
    extraArgs:
      - "--tensor-parallel-size"
      - "4"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--max-model-len"
      - "32768"
      - "--trust-remote-code"
    tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "genai"
        effect: "NoSchedule"

  # GLM-4.7-FP8 Model Service
  glm:
    enabled: true
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
    replicaCount: 1
    model:
      name: "zai-org/GLM-4.7-FP8"
      path: "/models/GLM-4.7-FP8"
    resources:
      limits:
        nvidia.com/gpu: 4
    service:
      port: 8003
    extraArgs:
      - "--tensor-parallel-size"
      - "4"
      - "--speculative-config.method"
      - "mtp"
      - "--speculative-config.num_speculative_tokens"
      - "1"
      - "--tool-call-parser"
      - "glm47"
      - "--reasoning-parser"
      - "glm45"
      - "--enable-auto-tool-choice"
      - "--served-model-name"
      - "glm-4.7-fp8"
      - "--trust-remote-code"
    tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "genai"
        effect: "NoSchedule"

# Ingress Configuration
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    cert-manager.io/cluster-issuer: "letsencrypt"
  
  hosts:
    - host: llm.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: llm
    - host: embedding.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: embedding
    - host: qwen.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: qwen
    - host: glm.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: glm

  tls:
    - hosts:
        - llm.aggpf.gpu-k8s.cloudcore-tu.net
        - embedding.aggpf.gpu-k8s.cloudcore-tu.net
        - qwen.aggpf.gpu-k8s.cloudcore-tu.net
        - glm.aggpf.gpu-k8s.cloudcore-tu.net
      secretName: vllm-tls-secret