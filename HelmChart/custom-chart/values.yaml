# Common configuration for all vLLM instances
common:
  image:
    repository: vllm/vllm-openai
    tag: v0.12.0
    pullPolicy: IfNotPresent

  persistence:
    enabled: true
    storageClass: "basic"
    accessModes:
      - ReadWriteMany
    size: 3000Gi
    mountPath: /models

  service:
    type: ClusterIP

  # Global HuggingFace Token
  hfToken: "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"

# Individual vLLM service definitions
# You can add more services here by adding a new key
services:
  # LLM Model Service
  llm:
    enabled: true
    replicaCount: 1
    model:
      name: "openai/gpt-oss-20b"
      path: "/models/gpt-oss-20b"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 80GB
    service:
      port: 8000
    # Additional arguments for vLLM serve command
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--max-num-batched-tokens"
      - "1024"
      - "--max-model-len"
      - "10240"
      - "--max-num-seqs"
      - "128"
      - "--async-scheduling"
      - "--dtype"
      - "bfloat16"

  # Embedding Model Service
  embedding:
    enabled: true
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-Embedding-8B"
      path: "/models/Qwen3-Embedding-8B"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 32GB
    service:
      port: 8001
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "512"
      - "--gpu-memory-utilization"
      - "0.9"
      - "--task"
      - "embed"
      - "--dtype"
      - "float16"

  # Qwen3-Coder-Next Model Service
  qwen:
    enabled: true
    image:
      repository: "vllm/vllm-openai"
      tag: "v0.15.0"
      pullPolicy: Always
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-Coder-Next"
      path: "/models/Qwen3-Coder-Next"
    resources:
      limits:
        nvidia.com/gpu: 4
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 80GB
    service:
      port: 8002
    extraArgs:
      - "--tensor-parallel-size"
      - "4"
      - "--gpu-memory-utilization"
      - "0.90"
      - "--tool-call-parser"
      - "qwen3_coder"
      - "--enable-auto-tool-choice"

  # GLM-4.7-FP8 Model Service
  glm:
    enabled: true
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
      pullPolicy: Always
    replicaCount: 1
    bootstrap: "pip install -U transformers"
    model:
      name: "zai-org/GLM-4.7-Flash"
      path: "/models/GLM-4.7-Flash"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 80GB
            # - key: kubernetes.io/hostname
            #   operator: In
            #   values:
            #   - "node1b-1"
    service:
      port: 8003
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--speculative-config.method"
      - "mtp"
      - "--speculative-config.num_speculative_tokens"
      - "1"
      - "--tool-call-parser"
      - "glm47"
      - "--reasoning-parser"
      - "glm45"
      - "--enable-auto-tool-choice"
      - "--served-model-name"
      - "glm-4.7-flash"
      - "--trust-remote-code"
      - "--dtype"
      - "bfloat16"
      - "--max-model-len"
      - "131072"
      - "--gpu-memory-utilization"
      - "0.93"
      - "--max-num-seqs"
      - "32"
         

  # Step-3.5-Flash Model Service
  step:
    enabled: true
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
      pullPolicy: Always
    replicaCount: 1
    # env vars removed - using BF16 instead of FP8 for better B200 compatibility
    model:
      name: "stepfun-ai/Step-3.5-Flash"
      path: "/models/Step-3.5-Flash"
    resources:
      limits:
        nvidia.com/gpu: 4
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 192GB
            - key: kubernetes.io/hostname
              operator: In
              values:
              - "node1b-1"
    service:
      port: 8004
    extraArgs:
      - "--gpu-memory-utilization"
      - "0.95"
      - "--served-model-name"
      - "step3p5-flash"
      - "--tensor-parallel-size"
      - "4"
      - "--enable-expert-parallel"
      - "--reasoning-parser"
      - "step3p5"
      - "--enable-auto-tool-choice"
      - "--tool-call-parser"
      - "step3p5"
      - "--trust-remote-code"
      - "--dtype"
      - "bfloat16"

  # Qwen3-ASR Model Service
  asr:
    enabled: false
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
      pullPolicy: Always
    bootstrap: "pip install -U flash-attn --no-build-isolation"
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-ASR-1.7B"
      path: "/models/Qwen3-ASR-1.7B"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 16GB
    service:
      port: 8005
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.85"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "128"
      - "--trust-remote-code"
      - "--dtype"
      - "float16"

  # Qwen3-TTS Model Service
  tts:
    enabled: false
    image:
      repository: "vllm/vllm-openai"
      tag: "v0.14.0"
      pullPolicy: Always
    bootstrap: |
      echo "Cloning vllm-omni repository..."
      git clone --depth 1 https://github.com/vllm-project/vllm-omni.git /tmp/vllm-omni
      echo "Installing vllm-omni from source..."
      pip install -e /tmp/vllm-omni
      echo "Installing qwen-tts..."
      pip install qwen-tts
      echo "Installing flash-attn..."
      pip install -U flash-attn --no-build-isolation
      echo "Bootstrap complete!"
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"
      path: "/models/Qwen3-TTS-12Hz-1.7B-CustomVoice"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 16GB
    service:
      port: 8006
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.85"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "128"
      - "--trust-remote-code"
      - "--dtype"
      - "float16"


# Ingress Configuration
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    cert-manager.io/cluster-issuer: "letsencrypt"
  
  hosts:
    - host: llm.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: llm
    - host: embedding.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: embedding
    - host: qwen.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: qwen
    - host: glm.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: glm
    - host: step.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: step
    - host: asr.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: asr
    - host: tts.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: tts

  tls:
    - hosts:
        - llm.aggpf.gpu-k8s.cloudcore-tu.net
        - embedding.aggpf.gpu-k8s.cloudcore-tu.net
        - qwen.aggpf.gpu-k8s.cloudcore-tu.net
        - glm.aggpf.gpu-k8s.cloudcore-tu.net
        - step.aggpf.gpu-k8s.cloudcore-tu.net
        - asr.aggpf.gpu-k8s.cloudcore-tu.net
        - tts.aggpf.gpu-k8s.cloudcore-tu.net
      secretName: vllm-tls-secret