# Common configuration for all vLLM instances
common:
  image:
    repository: vllm/vllm-openai
    tag: v0.12.0
    pullPolicy: IfNotPresent

  persistence:
    enabled: true
    storageClass: "basic"
    accessModes:
      - ReadWriteMany
    size: 1600Gi
    mountPath: /models

  service:
    type: ClusterIP

  # Global HuggingFace Token
  hfToken: "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"

# Individual vLLM service definitions
# You can add more services here by adding a new key
services:
  # LLM Model Service
  llm:
    enabled: true
    replicaCount: 1
    model:
      name: "openai/gpt-oss-20b"
      path: "/models/gpt-oss-20b"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 80GB
    service:
      port: 8000
    # Additional arguments for vLLM serve command
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--max-num-batched-tokens"
      - "1024"
      - "--max-model-len"
      - "10240"
      - "--max-num-seqs"
      - "128"
      - "--async-scheduling"
      - "--dtype"
      - "bfloat16"

  # Embedding Model Service
  embedding:
    enabled: true
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-Embedding-8B"
      path: "/models/Qwen3-Embedding-8B"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 32GB
    service:
      port: 8001
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "512"
      - "--gpu-memory-utilization"
      - "0.9"
      - "--task"
      - "embed"
      - "--dtype"
      - "float16"

  # Qwen3-235B Model Service
  qwen:
    enabled: false
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-235B-A22B-Instruct-2507-FP8"
      path: "/models/Qwen3-235B-A22B-Instruct-2507-FP8"
    resources:
      limits:
        nvidia.com/gpu: 4
    service:
      port: 8002
    extraArgs:
      - "--tensor-parallel-size"
      - "2"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--max-model-len"
      - "32768"
      - "--trust-remote-code"
    tolerations:
      - key: "dedicated"
        operator: "Equal"
        value: "genai"
        effect: "NoSchedule"

  # GLM-4.7-FP8 Model Service
  glm:
    enabled: true
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
      pullPolicy: Always
    replicaCount: 1
    bootstrap: "pip install -U transformers"
    model:
      name: "zai-org/GLM-4.7-Flash"
      path: "/models/GLM-4.7-Flash"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 80GB
            # - key: kubernetes.io/hostname
            #   operator: In
            #   values:
            #   - "node1b-1"
    service:
      port: 8003
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--speculative-config.method"
      - "mtp"
      - "--speculative-config.num_speculative_tokens"
      - "1"
      - "--tool-call-parser"
      - "glm47"
      - "--reasoning-parser"
      - "glm45"
      - "--enable-auto-tool-choice"
      - "--served-model-name"
      - "glm-4.7-flash"
      - "--trust-remote-code"
      - "--dtype"
      - "bfloat16"
      - "--max-model-len"
      - "131072"
      - "--gpu-memory-utilization"
      - "0.93"
      - "--max-num-seqs"
      - "32"
         

  # Kimi-K2.5 Model Service
  kimi:
    enabled: false
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
      pullPolicy: Always
    replicaCount: 1
    model:
      name: "moonshotai/Kimi-K2.5"
      path: "/models/Kimi-K2.5"
    resources:
      limits:
        nvidia.com/gpu: 8
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 141GB
            # - key: kubernetes.io/hostname
            #   operator: In
            #   values:
            #   - "node1b-1"
    service:
      port: 8004
    extraArgs:
      - "--tensor-parallel-size"
      - "8"
      - "--gpu-memory-utilization"
      - "0.95"
      - "--max-model-len"
      - "4096"
      - "--trust-remote-code"
      - "--enable-auto-tool-choice"
      - "--tool-call-parser"
      - "kimi_k2"
      - "--reasoning-parser"
      - "kimi_k2"
      - "--served-model-name"
      - "moonshotai/Kimi-K2.5"
      - "--kv-cache-dtype"
      - "fp8"

  # Qwen3-ASR Model Service
  asr:
    enabled: true
    image:
      repository: "vllm/vllm-openai"
      tag: "nightly"
      pullPolicy: Always
    bootstrap: "pip install -U flash-attn --no-build-isolation"
    replicaCount: 1
    model:
      name: "Qwen/Qwen3-ASR-1.7B"
      path: "/models/Qwen3-ASR-1.7B"
    resources:
      limits:
        nvidia.com/gpu: 1
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: memorysize
              operator: In
              values:
              - 16GB
    service:
      port: 8005
    extraArgs:
      - "--tensor-parallel-size"
      - "1"
      - "--gpu-memory-utilization"
      - "0.85"
      - "--max-model-len"
      - "4096"
      - "--max-num-seqs"
      - "128"
      - "--trust-remote-code"
      - "--dtype"
      - "float16"

# Ingress Configuration
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    cert-manager.io/cluster-issuer: "letsencrypt"
  
  hosts:
    - host: llm.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: llm
    - host: embedding.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: embedding
    - host: qwen.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: qwen
    - host: glm.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: glm
    - host: kimi.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: kimi
    - host: asr.aggpf.gpu-k8s.cloudcore-tu.net
      paths:
        - path: /
          pathType: Prefix
          serviceName: asr

  tls:
    - hosts:
        - llm.aggpf.gpu-k8s.cloudcore-tu.net
        - embedding.aggpf.gpu-k8s.cloudcore-tu.net
        - qwen.aggpf.gpu-k8s.cloudcore-tu.net
        - glm.aggpf.gpu-k8s.cloudcore-tu.net
        - kimi.aggpf.gpu-k8s.cloudcore-tu.net
        - asr.aggpf.gpu-k8s.cloudcore-tu.net
      secretName: vllm-tls-secret